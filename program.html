<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../assets/ico/favicon.ico">

    <title>Machine Learning in HPC Environments - Program </title>

    <!-- Bootstrap core CSS -->
    <link href="./dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="rrcf2014.css" rel="stylesheet">
  </head>
<!-- NAVBAR
================================================== -->
  <body>
    <div class="navbar-wrapper">
      <div class="container">

        <div class="navbar navbar-inverse navbar-static-top" role="navigation">
          <div class="container">

            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              	<a class="brand" href="index.html"> <img src="images/mlhpc.png" alt=""Machine Learning in HPC Environments"MLHPC2018"></a>
            </div>

            <div class="navbar-collapse collapse pull-right">
              <ul class="nav navbar-nav">
                <li class="active"><a href="index.html">Home</a></li>
                <li><a href="http://sc20.supercomputing.org/attend">Venue</a></li>
                <li><a href="index.html#important-dates">Important Dates</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Information<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="cfp.html">Call for Papers</a></li>
<!--                         <li><a href="keynotespeakers.html">Keynotes</a></li> -->
                        <li><a href="program.html">Program</a></li>
                    </ul>
                </li>
                <li class="dropdown">
          			<a href="#" class="dropdown-toggle" data-toggle="dropdown">Committees<b class="caret"></b></a>
          			<ul class="dropdown-menu">
            			<li><a href="organizingcommittee.html">Organizing Committee</a></li>
            			<li><a href="steeringcommittee.html">Steering Committee</a></li>
            			<li><a href="programcommittee.html">Program Committee</a></li>
            		</ul>
        		</li>
                <li><a href="index.html#contact">Contact</a></li>
              </ul>
            </div>
          </div>
        </div>

      </div>
    </div>


    <!-- Marketing messaging and featurettes
    ================================================== -->
    <!-- Wrap the rest of the page in another container to center all the content. -->

    <div class="container marketing">

      <!-- START THE FEATURETTES -->
      
      

      <hr class="featurette-divider">

      <div class="row featurette">
        <div class="col-md-7">
          <h2 class="featurette-heading">Workshop <span class="text-muted">Program</span></h2>

	<!--	<p class="lead"> <a href="https://sc18.supercomputing.org/session/?sess=sess221">Sunday</a></p> -->
	<p class="lead"> <a href="https://sc20.supercomputing.org/session/?sess=sess209">Thursday</a></p> 
	  
	<!--          <a href=https://denverconvention.com/uploads/pdf/Meeting_Room_Level_Map.pdf><p class="lead">Location: 502-503-504</p></a> -->
	<a href=https://sc20.supercomputing.org/session/?sess=sess115><p class="lead">Our workshop program is now available on the SC website.</p></a> 
	 <p class="lead">Date: Thursday November 12, 2020</p>
	<p class="lead">Time: 10:00am - 5:30pm</p> 
	<!--	<a href=papers/mlhpc2018_slides.zip><p class="lead">Slides (all slides up)</p></a>
		<a href=https://dl.acm.org/citation.cfm?id=3146347><p class="lead">Link to papers (now available)</p></a>
        </div>
	  -->
	<!--        <div class="col-md-5"> -->
          <!-- <img class="featurette-image img-responsive" data-src="holder.js/500x500/auto" alt="Generic placeholder image"> -->
        </div>
      </div>
      
      
       <hr class="featurette-divider">
      
      <div class="row featurette">
        
   
        <div class="col-md-7">
		<h2 class="lead">Workshop Introduction (10:00 am) <br> </h2>
	  <p class="lead"><span class="text-muted">Seung-Hwan Lim, Oak Ridge National Laboratory</span></p>
	</div>
	
      </div>
	       
      <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">Keynote: Lessons from Archives: Strategies for Collecting Sociocultural Data in Machine Learning
 10:10 am<br> </h2>
	  <p class="lead"><span class="text-muted">Timnit Gebru, Google</span></p>
          <p> A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. We argue that a new specialization should be formed within machine learning that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural machine learning.</p>
	<!--	<p><a href=papers/1_Keuper.pdf>[Presentation]</a></p> -->
	</div>
	</div>
      <hr class="featurette-divider">
	<div class="row featurette">      
        <div class="col-md-7">
		<h2 class="lead">Coffee Break (11:10 am)</h2>
        </div>
	</div>
	  
      <hr class="featurette-divider">
	    	<div class="row featurette">
		<div class="col-md-7">
		<h2 class="lead">EventGraD: Event-Triggered Communication in Parallel Stochastic Gradient Descent<br> </h2>
		<p class="lead"><span class="text-muted">Soumyadip Ghosh (University of Notre Dame), Vijay Gupta (University of Notre Dame)</span></p>
		<p>Communication in parallel systems consumes significant amount of time and energy which often turns out to be a bottleneck in distributed machine learning. In this paper, we present EventGrad - an algorithm with event-triggered communication in parallel stochastic gradient descent. The main idea of this algorithm is to modify the requirement of communication at every epoch to communicating only in certain epochs when necessary. In particular, the parameters are communicated only in the event when the change in their values exceed a threshold. The threshold for a parameter is chosen adaptively based on the rate of change of the parameter. The adaptive threshold ensures that the scheme can be applied to different models on different datasets without any change. We focus on data-parallel training of a popular convolutional neural network used for training the MNIST dataset and show that EventGrad can reduce the communication load by up to 70% while retaining the same level of accuracy.</p>
	<!-- <p><a href=papers/2_Cong.pdf>[Presentation]</a></p> -->
        	</div>
      		</div>
	  

	  
      <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">A Benders Decomposition Approach to Correlation Clustering<br></h2>
		<p class="lead"><span class="text-muted">Jovita Lukasik (University of Mannheim), Margret Keuper (University of Mannheim), Maneesh Singh (Verisk Analytics, Inc), Julian Yarkony (Verisk Analytics, Inc)</span></p>
		<p>We tackle the problem of graph partitioning for image segmentation using correlation clustering (CC), which we treat as an integer linear program (ILP). We reformulate optimization in the ILP so as to admit efficient optimization via Benders decomposition, a classic technique from operations research. Our Benders decomposition formulation has many subproblems, each associated with a node in the CC instance's graph, which can be solved in parallel. Each Benders subproblem enforces the cycle inequalities corresponding to edges with negative (repulsive) weights attached to its corresponding node in the CC instance. We generate Magnanti-Wong Benders rows in addition to standard Benders rows to accelerate optimization. Our Benders decomposition approach provides a promising new avenue to accelerate optimization for CC, and, in contrast to previous cutting plane approaches, theoretically allows for massive parallelization.</p>
	<!--	<p><a href=papers/3_Dryden.pdf>[Presentation]</a></p> -->
        </div>
        
      </div>
	  
      <hr class="featurette-divider">
	<div class="row featurette">    
        <div class="col-md-7">
          <h2 class="lead">Accelerating GPU-based Machine Learning in Python using MPI Library: A Case Study with MVAPICH2-GDR<br></h2>
		<p class="lead"><span class="text-muted">Seyedeh Mahdieh Ghazimirsaeed (Ohio State University), Quentin Anthony (Ohio State University),  Aamir Shafi (Ohio State University), Hari Subramoni (Ohio State University), Dhabaleswar K. (DK) Panda (Ohio State University) </span></p>
		<p>The growth of big data applications during the last decade has led to a surge in the deployment and popularity of machine learning libraries. On the other hand, the high performance offered by GPUs makes them well-suited for machine learning problems. To take advantage of the performance provided by GPUs for machine learning, NVIDIA has recently developed the cuML library. The cuML library is the GPU-counterpart of Scikit-learn and provides similar Pythonic interfaces while hiding the complexities of writing compute kernels for GPUs directly using CUDA. To support execution of machine learning workloads on Multi-Node Multi-GPU (MNMG) systems, the cuML library exploits NVIDIA Collective Communications Library (NCCL) as a backend for collective communications between the processes. On the other hand, MPI is a defacto standard for communication in HPC systems. Amongst various MPI libraries, MVAPICH2-GDR is the pioneer in optimizing communications for GPUs. This paper explores various aspects and challenges of providing MPI-based communication support for GPU accelerated cuML applications. More specifically, it proposes a Python API to take advantage of MPI-based communications for cuML applications. It also gives an in-depth analysis, characterization, and benchmarking of the cuML algorithms such as K-Means, Nearest Neighbors, Random Forest, and tSVD. Moreover, it provides a comprehensive performance evaluation and profiling study for MPI-based versus NCCL-based communication for these algorithms. The evaluation results show that the proposed MPI-based communication approach achieves up to 38%, 20%, 20%, and 26% performance gain for K-Means, Nearest Neighbors, Linear Regression, and tSVD, respectively on up to 32 GPUs. </p>
<!--		<p><a href=papers/4_Tsai.pdf>[Presentation]</a></p> -->
        </div>
	</div>
	  
 <!--    <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">Scheduling Optimization of Parallel Linear Algebra Algorithms Using Supervised Learning<br></h2>
		<p class="lead"><span class="text-muted">Gabriel Laberge (Polytechnique Montreal), Shahrzad Shirzad (Luisiana State University), Patrick Diehl (Luisiana State University), Hartmut Kaiser (Luisiana State University), Serge Prudhomme (Polytechnique Montreal), Adrian S. Lemoine (Luisiana State University)</span></p>
          <p>Linear algebra algorithms are used widely in a variety of domains, e.g. machine learning, numerical physics and video games graphics. For all these applications, loop-level parallelism is required to achieve high performance. However, finding the optimal way to schedule the workload between threads is a non-trivial problem because it depends on the structure of the algorithm being parallelized and the hardware the executable is run on. In the realm of Asynchronous Many Task runtime systems, a key aspect of the scheduling problem is predicting the proper chunk-size, where the chunk-size is defined as the number of iterations of a for-loop are assigned to a thread as one task. In this paper, we study the applications of supervised learning models to predict the chunk-size which yields maximum performance on multiple parallel linear algebra operations using the HPX backend of Blaze's linear algebra library. More precisely, we generate our training and tests sets by measuring performance of the application with different chunk-sizes for multiple linear algebra operations; vector-addition, matrix-vector-multiplication, matrix-matrix addition and matrix-matrix-multiplication. We compare the use of logistic regression, neural networks and decision trees with a newly developed decision tree based model in order to predict the optimal value for chunk-size. Our results show that classical decision trees and our custom decision tree model are able to forecast a chunk-size which results in good performance for the linear algebra operations. </p>
<p><a href=papers/5_Potok.pdf>[Presentation]</a></p>
        </div>  
	</div> -->
      <hr class="featurette-divider">
	<div class="row featurette">      
        <div class="col-md-7">
		<h2 class="lead">Lunch Break (12:55 PM ~ 2:30 PM)</h2>
        </div>
	</div>
 <!--     <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">Parallel Data-Local Training for Optimizing Word2Vec Embeddings for Word and Graph Embeddings<br></h2>
	  <p class="lead"><span class="text-muted"> Gordon E. Moon (Ohio State University), Denis Newman-Griffis (Ohio State University), Jinsung Kim (University of  Utah), Aravind Sukumaran-Rajam (Ohio State University), Eric Fosler-Lussier (Ohio State University), P. Sadayappan (University of Utah)</span></p>
		<p>The Word2Vec model is a neural network-based unsupervised word embedding technique widely used in applications such as natural language processing, bioinformatics and graph mining. As Word2Vec repeatedly performs Stochastic Gradient Descent (SGD) to minimize the objective function, it is very compute-intensive. However, existing methods for parallelizing Word2Vec are not optimized enough for data locality to achieve high performance. In this paper, we develop a parallel data-locality-enhanced Word2Vec algorithm based on Skip-gram with a novel negative sampling method that decouples loss calculation with positive and negative samples; this allows us to efficiently reformulate matrix-matrix operations for the negative samples over the sentence. Experimental results demonstrate our parallel implementations on multi-core CPUs and GPUs achieve significant performance improvement over the existing state-of-the-art parallel Word2Vec implementations while maintaining evaluation quality. We also show the utility of our Word2Vec implementation within the Node2Vec algorithm which accelerates embedding learning for large graphs.</p>
		<p><a href=papers/6_Schuman.pdf>[Presentation]</a></p> 
        </div>
	</div>
	  
      <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">Scalable Hyperparameter Optimization with Lazy Gaussian Processes<br></h2>
		<p class="lead"><span class="text-muted">Raju Ram (Fraunhofer Institute for Industrial Mathematics), Sabine Müller (Fraunhofer Institute for Industrial Mathematics), Franz-Josef Pfreundt (Fraunhofer Institute for Industrial Mathematics), Nicolas R. Gauger (Universityof Kaiserlautern), Janis Keuper (Fraunhofer Institute for Industrial Mathematics)</span></p>
		<p>Most machine learning methods require careful selection of hyper-parameters in order to train a high performing model with good generalization abilities. Hence, several automatic selection algorithms have been introduced to overcome tedious manual (try and error) tuning of these parameters. Due to its very high sample efficiency, Bayesian Optimization over a Gaussian Processes modeling of the parameter space has become the method of choice. Unfortunately, this approach suffers from a cubic compute complexity due to underlying Cholesky factorization, which makes it very hard to be scaled beyond a small number of sampling steps. 
			
			In this paper, we present a novel, highly accurate approximation of the underlying Gaussian Process. Reducing its computational complexity from cubic to quadratic allows an efficient strong scaling of Bayesian Optimization while outperforming the previous approach regarding optimization accuracy. First experiments show speedups of a factor of 162 in single node and further speed up by a factor of 5 in a parallel environment.</p>
<p><a href=papers/7_Camelo.pdf>[Presentation]</a></p> 
        </div>
	</div> -->

      <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">Keynote: Programming Systems of Data  (2:30 am)<br> </h2>
	  <p class="lead"><span class="text-muted">Michael Garland, NVIDIA</span></p>
          <p> Machine learning and data analysis thrive on mass quantities of data.  At the
same time, the cost of data distribution and movement is among the most
critical factors determining the performance of applications at scale.
Consequently, scalable high-performance machine learning and data analysis
requires software environments that support the careful management of data.
Whereas modern cloud systems provide data stores and services that help
support efficient delivery of data to applications, the tools at hand for
developers to efficiently manage distributed data within a running application
are considerably more limited.  It is particularly challenging to deliver
high-performance execution across distributed nodes while maintaining software
modularity and composability.  In this talk, I will focus on developments in
the design of scalable programming systems that help address these challenges
by providing data-centric interfaces that provide a convenient notation to the
developer and dynamic information to the runtime system tasked with scheduling
the application at peak efficiency.</p>
<!--		<p><a href=papers/1_Keuper.pdf>[Presentation]</a></p> -->
	</div>
	</div>	  	  
      <hr class="featurette-divider">
	<div class="row featurette">
	<div class="col-md-7">
          <h2 class="lead">Accelerate Distributed Stochastic Descent for Nonconvex Optimization with Momentum<br></h2>
		<p class="lead"><span class="text-muted">Guojing Cong (IBM Corporation), Tianyi Liu (Georgia Institute of Technology)</span></p>
		<p>Momentum method has been used extensively in optimizers for deep learn- ing. Recent studies show that distributed train- ing through K-step averaging has many nice properties. We propose a momentum method for such model averaging ap- proaches. At each individual learner level traditional stochas- tic gradient is applied. At the meta-level (global learner level), one momentum term is applied and we call it block mo- mentum. We analyze the convergence and scaling properties of such momentum methods. Our experimental results show that block momentum not only accelerates training, but also achieves better results.</p>
<!-- <p><a href=papers/7_Camelo.pdf>[Presentation]</a></p> -->
        </div>
        
      </div>
	    
	          <hr class="featurette-divider">
	<div class="row featurette">      
        <div class="col-md-7">
		<h2 class="lead">Coffee Break (4:00 PM - 4:15 PM)</h2>
        </div>
	</div> 
	  
      <hr class="featurette-divider">

<div class="row featurette">
        
        
        <div class="col-md-7">
          <h2 class="lead">High-bypass Learning: Automated Detection of Tumor Cells that Significantly Impact Drug Response<br></h2>
		<p class="lead"><span class="text-muted">Justin Wozniak (Argonne National Laboratory), Hyunseung Yoo (Argonne National Laboratory), Jamaludin Mohd-Yusof (Los Alamos National Laboratory), Bogdan Nicolae (Argonne National Laboratory), Richard Turgeon (Argonne National Laboratory), Nick Collier (Argonne National Laboratory), Jonathan Ozik (Argonne National Laboratory), Thomas Brettin (Argonne National Laboratory), Rick Stevens (Argonne National Laboratory)</span></p>
		<p>Machine learning in biomedicine is reliant on the availability of large, high-quality data sets. These corpora are used for training statistical or deep learning -based models that can be validated against other data sets and ultimately used to guide decisions. The quality of these data sets is an essential component of the quality of the models and their decisions. Thus, identifying and inspecting outlier data is critical for evaluating, curating, and using biomedical data sets. Many techniques are available to look for outlier data, but it is not clear how to evaluate the impact on highly complex deep learning methods. In this paper, we use deep learning ensembles and workflows to construct a system for automatically identifying data subsets that have a large impact on the trained models. These effects can be quantified and presented to the user for further inspection, which could improve data quality overall. We then present results from running this method on the near-exascale Summit supercomputer.</p>
<!-- <p><a href=papers/7_Camelo.pdf>[Presentation]</a></p> -->
        </div>
</div> 
      
	          <hr class="featurette-divider">

<div class="row featurette">
        
        
        <div class="col-md-7">
          <h2 class="lead">Deep Generative Models that Solve PDEs: Distributed Computing for Training Large Data-Free Models<br></h2>
		<p class="lead"><span class="text-muted">Sergio Botelho (RocketML Inc),  Ameya Joshi (New York University), Biswajit Khara (Iowa State University), Vinay Rao (RocketML Inc), Soumik Sarkar (Iowa State University), Chinmay Hegde (New York University), Santi Adavani (RocketML Inc),  Baskar Ganapathysubramanian (Iowa State University)</span></p>
		<p>Recent progress in scientific machine learning (SciML) has opened up the possibility of training novel neural network architectures that solve complex partial differential equations (PDEs). Several (nearly data free) approaches have been recently reported that successfully solve PDEs, with examples including deep feed forward networks, generative networks, and deep encoder-decoder networks. However, practical adoption of these approaches is limited by the difficulty in training these models, especially to make predictions at large output resolutions (greater or equal to 1024 x 1024). 

Here we report on a software framework for data parallel distributed deep learning that resolves the twin challenges of training these large SciML models – training in reasonable time as well as distributing the storage requirements. Our framework provides several out of the box functionality including (a) loss integrity independent of number of processes, (b) synchronized batch normalization, and (c) distributed higher-order optimization methods.

We show excellent scalability of this framework on both cloud as well as HPC clusters, and report on the interplay between bandwidth, network topology and bare metal vs cloud. We deploy this approach to train generative models of sizes hitherto not possible, showing that neural PDE solvers can be viably trained for practical applications. We also demonstrate that distributed higher-order optimization methods are 2-3 times faster than stochastic gradient-based methods and provide minimal convergence drift with higher batch-size.</p>
<!-- <p><a href=papers/7_Camelo.pdf>[Presentation]</a></p> -->
        </div>
</div> 
      
   
 

      <!-- FOOTER -->
      <footer>
      	<div style="width: 100%;overflow:auto;">
          <div style="float:left; width: 50%">

            <a name="contact"></a>
            <p><b>Contact:  Seung-Hwan Lim</b>, lims1 "at" ornl.gov</p>
            <p>&copy; 2020 Oak Ridge National Laboratory</p>
          </div>
           
          <div style="float:right;">
            <p>In cooperation with</p>
            <a class="brand" href="http://www.sighpc.org/"> <img src="images/sighpc_logo_72dpi.jpg" alt="Machine Learning in HPC Environments"></a>
          </div>
          
        </div>
        <p class="pull-right"><a href="#">Back to top</a></p>
      </footer>

    </div><!-- /.container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="./dist/js/bootstrap.min.js"></script>
    <script src="./assets/js/docs.min.js"></script>
  </body>
</html>
